import com.fasterxml.jackson.databind.JsonNode;
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.assigners.SlidingProcessingTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;

import java.util.Properties;

public class JsonEventAggregationExample {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Set up Kafka consumer with a JSON deserializer
        Properties properties = new Properties();
        properties.setProperty("bootstrap.servers", "localhost:9092");
        properties.setProperty("group.id", "my-consumer-group");

        FlinkKafkaConsumer<String> kafkaConsumer = new FlinkKafkaConsumer<>(
                "your-kafka-topic",
                new SimpleStringSchema(),
                properties
        );

        // Create a data stream from Kafka
        DataStream<String> stream = env.addSource(kafkaConsumer);

        // Parse JSON events into a custom event class
        DataStream<Event> eventStream = stream.map(jsonString -> {
            ObjectMapper objectMapper = new ObjectMapper();
            JsonNode jsonNode = objectMapper.readTree(jsonString);
            String key = jsonNode.get("key").asText();
            int value = jsonNode.get("value").asInt();
            return new Event(key, value);
        });

        // Apply windowing and aggregation
        DataStream<AggregateResult> aggregatedStream = eventStream
                .keyBy(Event::getKey)
                .window(SlidingProcessingTimeWindows.of(Time.seconds(10), Time.seconds(5)))
                .aggregate(new EventAggregateFunction());

        // Print the aggregated results to stdout
        aggregatedStream.print();

        // Execute the Flink job
        env.execute("Event Aggregation Example");
    }

    public static class Event {
        private String key;
        private int value;

        public Event(String key, int value) {
            this.key = key;
            this.value = value;
        }

        // getters, setters, and constructors

        // ...
    }

    public static class AggregateResult {
        private String key;
        private int sum;
        private int count;

        // getters, setters, and constructors

        // ...
    }

    public static class EventAggregateFunction implements AggregateFunction<Event, AggregateResult, AggregateResult> {
        @Override
        public AggregateResult createAccumulator() {
            return new AggregateResult("", 0, 0);
        }

        @Override
        public AggregateResult add(Event event, AggregateResult accumulator) {
            accumulator.setKey(event.getKey());
            accumulator.setSum(accumulator.getSum() + event.getValue());
            accumulator.setCount(accumulator.getCount() + 1);
            return accumulator;
        }

        @Override
        public AggregateResult getResult(AggregateResult accumulator) {
            return accumulator;
        }

        @Override
        public AggregateResult merge(AggregateResult a, AggregateResult b) {
            a.setSum(a.getSum() + b.getSum());
            a.setCount(a.getCount() + b.getCount());
            return a;
        }
    }
}

<dependencies>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-core</artifactId>
        <version>1.17.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-java</artifactId>
        <version>1.17.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-streaming-java_2.12</artifactId>
        <version>1.17.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-clients_2.12</artifactId>
        <version>1.17.0</version>
    </dependency>
    <dependency>
        <groupId>org.apache.flink</groupId>
        <artifactId>flink-json</artifactId>
        <version>1.17.0</version>
    </dependency>
</dependencies>




===================================================================================================================================
import com.fasterxml.jackson.databind.ObjectMapper;
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.api.common.serialization.SimpleStringSchema;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.functions.timestamps.AscendingTimestampExtractor;
import org.apache.flink.streaming.api.windowing.assigners.SlidingEventTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.connectors.kafka.FlinkKafkaConsumer;

import java.io.IOException;
import java.util.Properties;

public class KafkaDataAggregationExample {
    public static void main(String[] args) throws Exception {
        // Set up the execution environment
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Set the Kafka configuration
        Properties props = new Properties();
        props.setProperty("bootstrap.servers", "localhost:9092");
        props.setProperty("group.id", "flink-group");

        // Create a Kafka consumer
        FlinkKafkaConsumer<String> kafkaConsumer = new FlinkKafkaConsumer<>("topic", new SimpleStringSchema(), props);

        // Add the Kafka consumer as a data source
        DataStream<String> inputStream = env.addSource(kafkaConsumer);

        // Parse the JSON input and extract the necessary fields
        DataStream<MyData> dataStream = inputStream.map(json -> {
            ObjectMapper mapper = new ObjectMapper();
            try {
                MyData data = mapper.readValue(json, MyData.class);
                return data;
            } catch (IOException e) {
                e.printStackTrace();
                return null;
            }
        }).filter(data -> data != null);

        // Assign event time timestamps
        DataStream<MyData> timestampedStream = dataStream.assignTimestampsAndWatermarks(
                new AscendingTimestampExtractor<MyData>() {
                    @Override
                    public long extractAscendingTimestamp(MyData element) {
                        return element.getTimestamp();
                    }
                });

        // Apply the sliding window and keyBy operation
        DataStream<MyAggregatedData> aggregatedStream = timestampedStream
                .keyBy(MyData::getKey)
                .window(SlidingEventTimeWindows.of(Time.seconds(10), Time.seconds(5)))
                .aggregate(new MyAggregationFunction());

        // Print the aggregated results
        aggregatedStream.print();

        // Execute the job
        env.execute("Kafka Data Aggregation Example");
    }

    private static class MyData {
        private String key;
        private long timestamp;
        private int value;

        public String getKey() {
            return key;
        }

        public long getTimestamp() {
            return timestamp;
        }

        public int getValue() {
            return value;
        }
    }

    private static class MyAggregatedData {
        private String key;
        private int sum;
        private int count;

        public MyAggregatedData(String key, int sum, int count) {
            this.key = key;
            this.sum = sum;
            this.count = count;
        }

        public String getKey() {
            return key;
        }

        public int getSum() {
            return sum;
        }

        public int getCount() {
            return count;
        }
    }

    private static class MyAggregationFunction implements AggregateFunction<MyData, MyAggregatedData, MyAggregatedData> {
        @Override
        public MyAggregatedData createAccumulator() {
            return new MyAggregatedData("", 0, 0);
        }

        @Override
        public MyAggregatedData add(MyData value, MyAggregatedData accumulator) {
            accumulator.key = value.key;
            accumulator.sum += value.value;
            accumulator.count += 1;
            return accumulator;
        }

        @Override
        public MyAggregatedData getResult(MyAggregatedData accumulator) {
            return accumulator;
        }

        @Override
        public MyAggregatedData merge(MyAggregatedData a, MyAggregatedData b) {
            a.sum += b.sum;
            a.count += b.count;
            return a;
        }
    }
}
====================================================================================================================================
import org.apache.flink.api.common.functions.AggregateFunction;
import org.apache.flink.streaming.api.datastream.DataStream;
import org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;
import org.apache.flink.streaming.api.windowing.assigners.SlidingProcessingTimeWindows;
import org.apache.flink.streaming.api.windowing.time.Time;
import org.apache.flink.streaming.api.windowing.windows.TimeWindow;

public class SlidingWindowExample {
    public static void main(String[] args) throws Exception {
        // Set up the execution environment
        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();

        // Create a data stream from a source (replace with your own source)
        DataStream<Integer> dataStream = env.fromElements(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15);

        // Apply the sliding window operation
        DataStream<Double> resultStream = dataStream
                .windowAll(SlidingProcessingTimeWindows.of(Time.minutes(10), Time.minutes(1)))
                .aggregate(new SumAggregateFunction());

        // Print the result to stdout (replace with your own sink)
        resultStream.print();

        // Execute the Flink job
        env.execute("Sliding Window Example");
    }

    public static class SumAggregateFunction implements AggregateFunction<Integer, SumAccumulator, Double> {
        @Override
        public SumAccumulator createAccumulator() {
            return new SumAccumulator();
        }

        @Override
        public SumAccumulator add(Integer value, SumAccumulator accumulator) {
            accumulator.sum += value;
            accumulator.count++;
            return accumulator;
        }

        @Override
        public Double getResult(SumAccumulator accumulator) {
            return accumulator.sum / accumulator.count;
        }

        @Override
        public SumAccumulator merge(SumAccumulator a, SumAccumulator b) {
            a.sum += b.sum;
            a.count += b.count;
            return a;
        }
    }

    public static class SumAccumulator {
        public int sum;
        public int count;
    }
}
==============================



import org.apache.flink.api.java.tuple.Tuple2; import org.apache.flink.streaming.api.functions.windowing.ProcessWindowFunction; import org.apache.flink.streaming.api.windowing.assigners.TumblingEventTimeWindows; import org.apache.flink.streaming.api.windowing.evictors.CountEvictor; import org.apache.flink.streaming.api.windowing.time.Time; import org.apache.flink.streaming.api.windowing.triggers.CountTrigger; import org.apache.flink.streaming.api.windowing.windows.TimeWindow; import org.apache.flink.util.Collector; public class FirstDataEvictionExample { public static void main(String[] args) throws Exception { StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); // Create a stream of (key, value) tuples // ... KeyedStream<Tuple2<String, Integer>, String> keyedStream = input.keyBy(tuple -> tuple.f0); DataStream<String> result = keyedStream .window(TumblingEventTimeWindows.of(Time.seconds(5))) .evictor(CountEvictor.of(1)) // Evict after the first element .trigger(CountTrigger.of(1)) // Trigger after the first element .process(new ProcessWindowFunction<Tuple2<String, Integer>, String, String, TimeWindow>() { @Override public void process(String key, Context context, Iterable<Tuple2<String, Integer>> input, Collector<String> out) throws Exception { // Your processing logic here // Collect the elements and emit the result StringBuilder resultBuilder = new StringBuilder(); for (Tuple2<String, Integer> element : input) { resultBuilder.append(element.f0).append(":").append(element.f1).append(","); } out.collect(resultBuilder.toString()); } }); result.print(); env.execute("First Data Eviction Example"); } }

